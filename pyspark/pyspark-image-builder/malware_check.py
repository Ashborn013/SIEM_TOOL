from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col,
    lag,
    window,
    lit,
    collect_list,
    when,
    count,
    from_json,
    max as spark_max,
)
from pyspark.sql.types import TimestampType
from pyspark.sql.window import Window
from main import filter_logs_by_event_id, group_logs_by_date_latest, detect_special_privilege_logon, extract_new_process_creation_logs, detect_unusual_login_times 

def detect_process_termination(df):
    termination_logs = df.filter(df["event_id"] == "4689")
    if termination_logs.count() > 0:
        print("Process termination detected.")
        exe_logs = termination_logs.filter(col("message").contains(".exe"))
        exe_logs.select("message").distinct().show(truncate=False)
    else:
        print("No process termination detected.")

def detect_suspicious_network_activity(df):
    network_logs = filter_logs_by_event_id(df, 5156)
    source_port = network_logs["winlog"]["event_data"]["SourcePort"]
    dest_port = network_logs["winlog"]["event_data"]["DestPort"]
    suspicious_ips = network_logs.filter(
        (col("SourceAddress") == "127.0.0.1") |
        (col("DestAddress") == "127.0.0.1") |
        (col("SourcePort") == source_port) |
        (col("DestPort") == dest_port)
    )
    if suspicious_ips.count() > 0:
        print("Suspicious network activity detected.")
        suspicious_ips.show()
    else:
        print("No suspicious network activity detected.")


def detect_credential_access(df):
    credential_logs = filter_logs_by_event_id(df, 5379)
    if credential_logs.count() > 0:
        print("Credential access detected.")
        credential_logs.show()
    else:
        print("No credential access detected.")

def detect_object_access(df):
    object_access_logs = filter_logs_by_event_id(df, 4663)
    if object_access_logs.count() > 0:
        print("Object access detected.")
        object_access_logs.show()
    else:
        print("No object access detected.")

def detect_permission_changes(df):
    permission_logs = filter_logs_by_event_id(df, 4670)
    if permission_logs.count() > 0:
        print("Permission changes detected.")
        permission_logs.show()
    else:
        print("No permission changes detected.")

def detect_privilege_service(df):
    pservice_logs = filter_logs_by_event_id(df, 4673)
    if pservice_logs.count() > 0:
        print("Privilege service detected.")
        pservice_logs.show()
    else: 
        print("No privilege service detected.")

def detect_group_membership(df):
    group_logs = filter_logs_by_event_id(df, 4627)
    if group_logs.count() > 0:
        print("Group membership detected.")
        group_logs.show()
    else:
        print("No group membership detected.")

def detect_sec_grp_membership(df):
    sec_grp_logs = filter_logs_by_event_id(df, 4799)
    if sec_grp_logs.count() > 0:
        print("Security group membership detected.")
        sec_grp_logs.show()
    else:
        print("No security group membership detected.")

def detect_attempt_to_duplicate_handle_to_object(df):
    handle_logs = filter_logs_by_event_id(df, 4690)
    if handle_logs.count() > 0:
        print("Attempt to duplicate handle to object detected.")
        handle_logs.show()
    else:
        print("No attempt to duplicate handle to object detected.")

# Correlation function
def correlate_logs(df):
    print("Starting log correlation...")
    detect_process_termination(df)
    detect_credential_access(df)
    detect_object_access(df)
    detect_permission_changes(df)
    detect_suspicious_network_activity(df)
    detect_privilege_service(df)
    detect_group_membership(df)
    detect_sec_grp_membership(df)
    detect_attempt_to_duplicate_handle_to_object(df)

# Main function to process logs
def checkmalware(spark, file_path):
    df = spark.read.json(file_path)

    df_selected = df.select(
        "@timestamp",
        "log",
        "message",
        "ecs",
        "event",
        col("agent").getItem("name").alias("name"),
        col("agent").getItem("id").alias("id"),
        col("agent").getItem("type").alias("type"),
        col("winlog").getItem("event_id").alias("event_id"),
        col("host").getItem("hostname").alias("hostname"),
    )

    df_selected = df_selected.withColumn(
        "@timestamp", col("@timestamp").cast(TimestampType())
    )

    correlate_logs(df_selected)
