from utils import filter_logs_by_event_id
from pyspark.sql.functions import (
    col,
    date_format,
    max as spark_max,
)


def bonzi_malware_correlation(df):
    if df is None or df.rdd.isEmpty():
        print("Input DataFrame is empty or None, skipping rule.")
        return
    # df = df.withColumn("event", to_json(df["event"]))
    df_latest_day = group_logs_by_date_latest(df)
    event_ids = [4624, 4673, 5158, 4627, 5379, 4672, 4689, 4663, 4670, 4799, 4656, 4688, 5156, 4690, 4658, 4703]

    union_df = None
    # union_df = df.dropDuplicates()
    for event_id in event_ids:
        df_filter = filter_logs_by_event_id(df_latest_day, event_id)
        if union_df is None:
            union_df = df_filter
        else:
            df1= df_filter.dropDuplicates()
            union_df1 = union_df.union(df1) # Union of all filtered DataFrames

    if union_df1 is not None:
        union_df1.show(truncate=False)
        # union_df1.write.json("bonzi_malware_correlation.json") # move to db
        print("Malware detected")
    else:
        print("No malware detected")

def group_logs_by_date_latest(df):
    df_with_day = df.withColumn("day", date_format(col("@timestamp"), "yyyy-MM-dd"))
    latest_day = df_with_day.agg(spark_max("day")).collect()[0][0]
    # print(latest_day)
    df_latest_day = df_with_day.filter(col("day") == latest_day)
    return df_latest_day
